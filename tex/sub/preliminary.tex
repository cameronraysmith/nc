\section{ICOB: Theory of computation in cell cultured neural networks}
\section{Section 1: Contact information for the Israeli PI}
\noindent Prof. Elisha Moses, Weizmann Institute of Science, +917-8-934-3139, \href{mailto:elisha.moses@weizmann.ac.il}{elisha.moses@weizmann.ac.il}
%\begin{tabular}{ l l }
%Prof. Elisha Moses & Prof. Aviv Bergman\\
%Physics of Complex Systems & Department of Systems and Computational Biology\\
%Weizmann Institute of Science & Albert Einstein College of Medicine\\
%PO Box 26 & Price Center Rm. 153\\
%Rehovot 76100 & Bronx, NY 10461\\
%Israel & USA\\
%Phone +917-8-934-3139 & Phone +1-718-678-1063\\
%Email elisha.moses@weizmann.ac.il & Email aviv@einstein.yu.edu\\
%\end{tabular}

\section{Section 2: IOS Program Justification}
\label{sec:justification}
\noindent BIO/IOS Neural Systems NSF 11-572. Our proposed work is most appropriate for the IOS neural systems section as it deals with fundamental questions relating to the computational capabilities of the central nervous systems of living organisms. The associated questions we intend to address are fundamental to an integrative understanding of structure-function physiology and co-evolution in all organisms with nervous systems and the evolutionary predecessors that gave rise to them. With respect to nervous system \emph{organization}, we intend to deepen previous explorations into the interaction of developmental and environmental constraints on the architecture and complementary computational capacity of networks of living neurons. Focusing on flexible abstract models of computation, rather than those specific to digital electronics, will enable us to investigate potentially novel computational potential implemented via architectures that differ from digital electronics in their apparent ability to take advantage of synergy between stochastic and deterministic properties. We may therefore, in addition to contributing to fundamental biological understanding, identify computational principles unique to natural neural networks. It is essential in this project to link theoretical, computational and experimental methods of inquiry. This is necessary to address questions we ask regarding co-evolution and co-development between underlying biological organization and computational tasks such structures are capable of performing in a variety of environmental conditions. We have assembled a culturally diverse and interdisciplinary team of experienced and developing scientists to enable the success of the research program we propose.

\section{Section 3: Research description}

\subsection{Motivating questions}
One question regarding the brain is, why neurons that perform astoundingly complex calculations become inefficient at computation once they are grown outside of it. In particular, once one takes hippocampal neurons out of the rat brain and cultures them on a dish, they lose their individuality. Living neuronal cultures are characterized by all-or-none network bursts, where practically all the neurons fire simultaneously, with varying degrees of synchrony. While in the brain they can compute the trajectory of the animal in a maze, but as an ensemble grown out of it they seem unable to perform new computations, and can only carry very few bits of information \cite{Feinerman2006}. How is it then that individual neurons are organized in the brain as a splendid computer, but in the dish their computational repertoire seems limited? Obviously, their environment and growth conditions have changed, but can we isolate those elements that are necessary, and perhaps sufficient, to support computation by a living neuronal network? In an effort to tackle such questions, we have embarked on an investigation of the computational abilities of living neuronal networks, and propose here to expand this investigation both theoretically and experimentally.

On the conceptual side, a number of deep questions arise. What are the structural properties at the level of networks of neurons,
modules of networks of neurons, and perhaps higher order forms of
organization necessary to support the capacity for
abstraction, which is fundamental to computation \cite{Abelson1996} and may likewise be fundamental to
\href{http://en.wikipedia.org/wiki/Cognitive\_architecture}{cognitive
architecture}, development, and function \cite{Tenenbaum2011}? Given that we have shown boolean
logic devices capable of being implemented {\em in vitro} using
neurons \cite{Feinerman2008}, is it possible to biologically engineer analogous
devices capable of performing computations that have natural embeddings
within \href{http://en.wikipedia.org/wiki/First-order\_logic}{first} or
\href{http://en.wikipedia.org/wiki/Higher\_order\_logic}{higher-order
logic}?

We plan to experimentally develop a number of novel logical devices, as well as use a number of new technologies that we have acquired for the construction of such computational constructs. Notable among these are the optogenetic toolbox that has recently become available. Our access to this toolbox includes light induced neuronal excitation using channelrhodopsins (courtesy of the Deisseroth lab at Stanford and the Yizhar lab at Weizmann) as well as genetically encoded fluorescent labels for optical imaging of neuronal excitation (courtesy of the Cohen lab at Harvard).

Evolutionary forces have shaped the architecture, connectivity as well as the input that neurons grow with inside the brain, all of which are involved in its apparently emergent computational capability. Biological computation in general is set apart from that of electronic computers by the fact that the `hardware' (e.g. the cell or the organism) co-evolved with the `software' (DNA or the brain respectively). As a result, our `software' is naturally co-optimized for the associated `hardware', while in the computer world that is not necessarily, if ever, the case. This insight leads us to propose experiments that will connect the neuronal network with the `real' world, and  allow the structure and function to co-evolve in a range of different environmental conditions.

\subsection{\emph{Natural} models of computation in living neural networks}

Since the development of the electronic digital computer, which makes
use of the digital abstraction \cite{Ward1989} from analog electrical circuits
there has been a close heuristic association between boolean logic and
computation. However, developments in formal logic over the past century
have been largely motivated by its applications to computation and the
theory of programming languages that go beyond boolean logic to support additional forms of abstraction. The capacity to support abstraction mirroring various systems of
formal logic is the primary way in which languages are compared \cite{Abelson1996}. Electronic devices that are capable
of supporting such forms of abstraction provide a concrete physical
instantiation of the ideas inherent to the logical systems they are
designed to faithfully implement.

Neuronal logic devices (NLDs) represent an alternative physical modality
to digital electronics for the purpose of performing computation.
However, rather than attempting to directly parallel the history of the
development of digital electronics via the digital abstraction,
high-level descriptions of computation, such as the
$\lambda$-calculus, serve as a specification
of computation that is agnostic to the physical modality of
implementation. If the specification that a neuronal computation device
should be capable of implementing the
$\lambda$-calculus, which is Turing complete formal system, a natural first step
toward this broad goal is to investigate simple neuronal systems that
are capable of performing well-defined computations that require a
capacity for first- or some higher-order logic.

One challenge to be addressed experimentally is whether such a system can be implemented reliably using central nervous system (CNS) neurons, which are individually unreliable components. We have shown previously that complex devices such as a diode, an oscillator and an AND-gate can be engineered using CNS neurons grown on particular geometric configurations \cite{Feinerman2008}. While a complicated configuration of NAND gates could, in principle, enable the construction of a universal Turing machine, this may be difficult to engineer with previously developed methods. Alternative geometric configurations and measurement patterns coupled with developmental selection will enable the identification of configurations of neurons capable of performing more difficult computations.

To develop higher order computational devices, we will turn to the experimental techniques of microfluidics and optogenetics \cite{Yizhar2011,Kralj2012}.
It is now possible to monitor with optical means the electrical activity of the network without any collateral damage caused by the fluorescent dye. This is done by genetic incorporation of a fluorescent voltage-indicating protein into the neuron. It is furthermore possible to excite a region of the network optically by activating photosensitive channels that are genetically embedded as well. On top of this, the propagation velocity of a signal inside a one-dimensional neuronal network of the type we are using is constant, and can be reliably predicted. Thus it becomes possible to identify activity in one part of a device, and then excite another region co-incidentally with the arrival of the signal into that area. Different time delays, with the activation before, during or after the arrival of the synaptic input will allow the creation of several neuronal learning scenarios, and the comparison to learning in organisms with brains.

\subsection{A higher-order function to be implemented as a higher-order NLD}

The $\lambda$-calculus notation is helpful in
order to define any higher order function. This notation may be
implicitly familiar to users of imperative programming languages as ``anonymous functions''. We provide an informal set of examples necessary to clarify our work. Complete details can be found in \cite{Barendregt1985}. We can define a
standard binary boolean function like the ``and'' function as $\lambda x. \lambda y.(\wedge\,\,x\,\,y)$. Such a lambda expression can apparently be applied to any inputs;
however the inputs to such a function are not necessarily restricted to
booleans unless we infer that the standard logical operator
``$\wedge$'' only accepts boolean arguments.
Note that we have used the
\href{http://en.wikipedia.org/wiki/Polish\_notation}{prefix or Polish
notation} for the $\wedge$ operator, which
in the perhaps more common infix notation is written with its arguments
flanking the operator as $x \wedge y$ to mean
``$x$ and
$y$''. We can provide explicit type
annotations for the bound variables $x$
and $y$, which indicate the types of
the arguments $\lambda x:bool. \lambda y:bool.(\wedge\,\,x\,\,y)$. We can now also provide a type annotation for this expression as a whole
\begin{equation}\label{eq:boolfulltype}
[\lambda x:bool. \lambda y:bool.(\wedge\,\,x\,\,y)]:[bool \rightarrow bool \rightarrow bool]
\end{equation}
Depending upon conventions with respect to
\href{http://en.wikipedia.org/wiki/Currying}{currying}, one could read
the type annotation as ``the function that takes two arguments each of
type $bool$ and returns a values of type
$bool$'' or ``the function that takes an
argument of type $bool$ and returns a
function that takes an argument of type
$bool$ and returns a value of type
$bool$''. The first formulation may be
easier to read, but the second is standard as a result of the design of
functional programming languages.

Now we can imagine that if we wish to abstract from the particular
binary boolean function implied by the
$\wedge$ operator, we need to introduce a
functional variable, whose type will be explicitly denoted for
concreteness despite the fact that it could be inferred, for which this
operator can be substituted. Doing this results in the following second
order function
\begin{multline}\label{eq:lamhobf}
[\lambda f:(bool \rightarrow bool \rightarrow bool).
\lambda x:bool. \lambda y:bool.(f\,\,x\,\,y)]
\\
:[(bool \rightarrow bool \rightarrow bool)
\rightarrow bool \rightarrow bool \rightarrow bool]
\end{multline}
This function is intended to be read, in the less verbose uncurried
form, as ``the function that takes as its first argument, a function
that takes two boolean values as arguments and returns a boolean value,
and, as its second and third arguments, two boolean values, and returns a
boolean value''. It is perhaps remarkable that this simple abstraction
is now capable of implementing any of the 16 possible boolean functions,
provided that the proper binary boolean operator is submitted as the
first argument to this function. The statement of this
function in terms of $\lambda$-calculus is agnostic to any physical implementation capable of realizing extensionally
equivalent behavior.

To make this more concrete, we can very simply implement the above
function in any programming language.
An implementation in the programming language OCaml appears in listing \ref{camlhobf}.
\begin{lstlisting}[caption={a higher order boolean function},label=camlhobf]
let hobf = fun (bf : ('a 'a bool)) (i1 : bool) (i2 : bool)
	       -> bf i1 i2
\end{lstlisting}
What is required in order to evaluate this function are corresponding
implementations of boolean functions to be substituted either for
$f$ in the lambda calculus notation or
for bf in terms of the corresponding OCaml implementation. For example
we can implement the XOR function using pattern matching to define a
truth table as shown in listing \ref{camlXOR}.
\begin{lstlisting}[caption={implementation of an XOR boolean operator},label=camlXOR]
let xor p1 p2 = match (p1, p2)
with (false, false) -> false
   | (false,  true) -> true
   | ( true, false) -> true
   | ( true,  true) -> false
\end{lstlisting}
Other binary boolean functions are implemented in an analogous fashion.
In order to evaluate hobf we then simply provide the name of a binary
boolean function and two boolean values. For example $hobf \,\, xor \,\, 0 \,\, 0=0$ and $hobf \,\, xor \,\, 0 \,\, 1=1$.

\subsection{Assessment of abstraction potential in NLDs}
An important consideration is to state precisely some criterion for
determining that a particular NLD has achieved potential for an explicit
form of abstraction such as is indicated in the relationship between
expressions \ref{eq:boolfulltype} and
\ref{eq:lamhobf}. In abstracting the binary boolean
operator $\wedge$ to the functional variable
$f$, which is the fundamental
transformation enabling the derivation of
\ref{eq:lamhobf} from
\ref{eq:boolfulltype}, we imply that any physical
implementation must take at least three rather than two inputs and the
first of these must specify a particular binary boolean operator to
apply to the latter two boolean input values. This roughly means that
any system that at least partially implements the lambda expression or
function specified in equation \ref{eq:lamhobf} and
listing \ref{camlhobf} respectively , must be capable of
interpreting the concept of ``selection from a set'' whose size is
determined by the subset of the 16 binary boolean operators that is
already implemented in a lower-level form. Indeed another representation
of equation \ref{eq:lamhobf} as a partial or total
set function could be written as $hobf : Hex \times Bool \times Bool \rightarrow Bool$
where we interpret $Bool$ and
$Hex$ as two and sixteen element sets
respectively. This point of view makes clear that the capacity for
selection from two element sets is already apparent in the binary
boolean operator written as a set function
$\wedge : Bool \rightarrow Bool$. The difference between these is
that the system must implement the typing constraints necessary to
distinguish a set that takes on sixteen possible values from one that
takes on two. For example, in the application of the function
$hobf$ the following evaluate as expected
given the definition: $hobf \,\, \wedge \,\, 1\,\, 0=0$ or
$hobf \,\, \wedge \,\, 1\,\, 1=1$. However, what is to be expected
given inputs such as: $hobf \,\, 1 \,\, \wedge\,\, 0$ or
$hobf \,\, 1 \,\, 1\,\, \wedge$? In these cases an output type
intuitively associated to $error$ is
required to indicate that the realization of a system implementing
equation \ref{eq:lamhobf} has indeed correctly
implemented the necessary typing constraints to claim that the function
has been realized. In the case of neuronal logic devices, this
alternative output should differ in a measurable way from those
associated to $1$ and $0$.

\subsection{Neural network simulations toward NLD design}
In order to attempt to determine geometric configurations that may support the growth of patterned cultures of neurons capable of performing the task of switching between AND and OR gate functionality, we created an artificial neural network (ANN) model. Figure \ref{fig:hintDiag} shows Hinton diagrams for the weights of sample 3-layer 3-input 1-output feedforward ANNs trained on data representing the AND/OR gate switching task. Figure \ref{fig:nnCG} shows a heatmap where neural networks are clustered according to their respective weights in order to determine the degree to which networks capable of performing the and/or gate switching task may be clustered into a reduced number of strategies.

\begin{figure}
\begin{center}
\begin{tabular}{cccc}
\subfloat[]{\includegraphics[width=0.2\textwidth,page=1]{../fig/combined.pdf}} 
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=2]{../fig/combined.pdf}}
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=3]{../fig/combined.pdf}} 
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=4]{../fig/combined.pdf}}\\
   \subfloat[]{\includegraphics[width=0.2\textwidth,page=5]{../fig/combined.pdf}} 
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=6]{../fig/combined.pdf}}
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=7]{../fig/combined.pdf}} 
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=8]{../fig/combined.pdf}}\\
   \subfloat[]{\includegraphics[width=0.2\textwidth,page=9]{../fig/combined.pdf}} 
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=10]{../fig/combined.pdf}}
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=11]{../fig/combined.pdf}} 
   & \subfloat[]{\includegraphics[width=0.2\textwidth,page=12]{../fig/combined.pdf}}
\end{tabular}
\end{center}
\caption{Hinton diagrams of feedforward neural network weights for neural networks trained on the and/or gate switching task.}\label{fig:hintDiag}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{../fig/dendrogram.pdf}
\end{center}
\caption{A clustered heatmap showing the relationship between low error strategies that emerge in training 50 neural networks on the and/or gate switching task.}\label{fig:nnCG}
\end{figure}

\subsection{From NLDs to the CNS and principles of cognition}

As noted above, we associate the failure of a cultured neuronal network to produce a valid computation with the fact that it is grown out of context, out of its natural surroundings. By using geometric constraints, we have shown that we can coax some of the networks connections to go in the direction we engineer, and a modicum of computation is restored \cite{Feinerman2008}. However, the idea that the `software' is growing without the presence of the `hardware' it is accustomed to, leads us to the idea of co-culturing a different set of neurons along with the regular hippocampal ones. The obvious choice for us is that of sensory neurons, since those are the subset of neurons that communicate between the body of the organism and its brain. In this way we hope to recreate an information channel that exists in the developing brain, allowing neurons to attain functional connectivity according to the inputs it receives.
We are thus communicating with a number of groups that study pain, with the intention of extracting sensory neurons that synapse onto CNS neurons to convey the signals associated with stress and pain. This is a natural initial choice, since the preception of pain signals is fundamental to survival in many cases and thus neurons may have evolved to be robustly sensitive to such signals.

From the point of view that identifies the capacity for abstraction with
computation, the investigation of neuronal logic devices capable of such
function provides a framework in which various hypotheses from cognitive
science could begin to be evaluated at the level of well-defined neural
circuits. By attempting to isolate minimal implementation criteria, this
approach may serve to complement, enable simpler explanations of, or
identify paradoxical results derived from studies that treat whole
brains and their associated sensory apparatus as their object of study
\cite{McClelland2010,Griffiths2010}.

\section{Section 4: Role and expertise of the PIs}
Both Profs. Aviv Bergman and Elisha Moses have significant experience working in collaborative teams of experimentalists and theorists. The PIs intend this project to involve strong interaction between experiment and theory. Prof. Bergman will lead the theoretical component of this project. Prof. Bergman's expertise spans a number of theoretical areas including artificial neural networks, dynamical systems, mathematical evolutionary biology, and systems biology. The Bergman lab will develop the computational platform and analytical tools to predict neuronal architecture-function relationships, which in turn will help guide the construction of environmental conditions to guide the development of natural neural networks capable of performing fundamental computational tasks. Prof. Moses will lead the experimental component of the project, heading a laboratory that focuses on the growth and measurement of neuronal activity in networks grown from CNS neurons from rat and mouse brain. The lab has pioneered the design of complex neuronal logical devices, has generated several new experimental paradigms combining nonlinear dynamics and statistical physics with biological physics, and has participated in the formation of novel theoretical models for engineered neuronal networks.

\section{Section 5: Educational involvement}
In the Bergman lab, three Ph.D. students (Mr. Cameron Smith, Mr. Daniel Biro and Mr. Ximo Pechaun) will be involved in developing the theoretical aspect of this project in close interaction with the PI. In the Moses lab two students (1 Ph.D., Ms. Shani Stern and 1 M.Sc., to be hired) and a postdoctoral fellow (Dr. Yaron Penn) will be involved. Students and postdoctoral fellows in the Bergman and Moses labs will interact on a weekly basis via video conference. More extensive interaction will be fostered by an exchange program that we plan to engage in at crucial theory-experiment integration stages throughout the project. Virtual interaction among all involved, including the public, will be fostered by an open online \href{http://en.wikipedia.org/wiki/Wiki}{Wiki} that will be used to organize and collaborate on this project and, more generally, support the movement for \href{http://en.wikipedia.org/wiki/Open\_notebook\_science}{Open Notebook Science}. All computer code will be made open source in accordance with the \href{http://opensource.org/licenses/MIT}{MIT license} and the codebase history will be available to the public free and in real-time on \href{http://www.github.com}{github}. Despite the fact that both labs already combine theory and experiment, they do so in very different ways and exposure to each of these models for combining theory and experiment will be crucial for these students as they move on to make very important decisions in their careers with respect to postdoctoral training and ultimately building labs of their own.